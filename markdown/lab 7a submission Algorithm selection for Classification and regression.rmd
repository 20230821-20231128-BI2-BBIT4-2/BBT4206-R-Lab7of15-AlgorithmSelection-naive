title: "Business Intelligence Lab Submission Markdown"
author: "naive"
date: "4/10/2023"
output:
  github_document: 
    toc: yes
    toc_depth: 4
    fig_width: 6
    fig_height: 4
    df_print: default
editor_options:
  chunk_output_type: console
---

# Student Details

+---------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| **Student ID Numbers and Names of Group Members** | *\<list one student name, class group (just the letter; A, B, or C), and ID per line, e.g., 123456 - A - John Leposo; you should be between 2 and 5 members per group\>* |
|                                                   |                                                                                                                                                                          |
|                                                   | 1.  135575 - B - Dennis Nzioki.                                                                                                                                             |
|                                                   |                                                                                                                                                                          |
|                                                   | 2.  134645 - B - Vivean Lydiah                                                                                                                                             |
|                                                   |                                                                                                                                                                          |
|                                                   | 3.  134765 - B - Nicholas Munene                                                                                                                                             |
|                                                   |                                                                                                                                                                          |
|                                                   | 4.  131653- B - Terry Joan                                                                                                                                             |
|                                                   |                                                                                                                                                                          |
|                                                   | 5.  124428 - B - Eston Gichuhi                                                                                                                                              |
+---------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| **GitHub Classroom Group Name**                   | *\<specify the name of the team you created on GitHub classroom\>*                                                                                                       |
+---------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| **Course Code**                                   | BBT4206                                                                                                                                                                  |
+---------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| **Course Name**                                   | Business Intelligence II                                                                                                                                                 |
+---------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| **Program**                                       | Bachelor of Business Information Technology                                                                                                                              |
+---------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| **Semester Duration**                             | 21^st^ August 2023 to 28^th^ November 2023                                                                                                                               |
+---------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

# Setup Chunk

We start by installing all the required packages
We start by installing all the required packages

```{r}
if (require("languageserver")) {
  require("languageserver")
} else {
  install.packages("languageserver", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}
if (require("stats")) {
  require("stats")
} else {
  install.packages("stats", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}

## mlbench ----
if (require("mlbench")) {
  require("mlbench")
} else {
  install.packages("mlbench", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}

## caret ----
if (require("caret")) {
  require("caret")
} else {
  install.packages("caret", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}

## MASS ----
if (require("MASS")) {
  require("MASS")
} else {
  install.packages("MASS", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}

## glmnet ----
if (require("glmnet")) {
  require("glmnet")
} else {
  install.packages("glmnet", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}

## e1071 ----
if (require("e1071")) {
  require("e1071")
} else {
  install.packages("e1071", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}

## kernlab ----
if (require("kernlab")) {
  require("kernlab")
} else {
  install.packages("kernlab", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}

## rpart ----
if (require("rpart")) {
  require("rpart")
} else {
  install.packages("rpart", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}
```
#Linear Algorithms
## Linear regression
### 1Linear Regression using Ordinary Least Squares without caret ----
The lm() function is in the stats package and creates a linear regression
model using ordinary least squares (OLS).
```{r}

#### Load and split the dataset ----
data("ToothGrowth")

# Define an 80:20 train:test data split of the dataset.
train_index <- createDataPartition(ToothGrowth$dose,
                                   p = 0.8,
                                   list = FALSE)
ToothGrowth_train <- ToothGrowth[train_index, ]
ToothGrowth_test <- ToothGrowth[-train_index, ]

#### Train the model ----
ToothGrowth_model_lm <- lm(dose ~ ., ToothGrowth_train)

#### Display the model's details ----
print(ToothGrowth_model_lm)

#### Make predictions ----
predictions <- predict(ToothGrowth_model_lm, ToothGrowth_test[, 1:2])

#### Display the model's evaluation metrics ----
##### RMSE ----
rmse <- sqrt(mean((ToothGrowth_test$dose - predictions)^2))
print(paste("RMSE =", sprintf(rmse, fmt = "%#.4f")))

##### SSR ----
# SSR is the sum of squared residuals (the sum of squared differences
# between observed and predicted values)
ssr <- sum((ToothGrowth_test$dose - predictions)^2)
print(paste("SSR =", sprintf(ssr, fmt = "%#.4f")))

##### SST ----
# SST is the total sum of squares (the sum of squared differences
# between observed values and their mean)
sst <- sum((ToothGrowth_test$dose - mean(ToothGrowth_test$dose))^2)
print(paste("SST =", sprintf(sst, fmt = "%#.4f")))

##### R Squared ----
# We then use SSR and SST to compute the value of R squared.
# The closer the R squared value is to 1, the better the model.
r_squared <- 1 - (ssr / sst)
print(paste("R Squared =", sprintf(r_squared, fmt = "%#.4f")))

##### MAE ----
# MAE is expressed in the same units as the target variable, making it easy to

absolute_errors <- abs(predictions - ToothGrowth_test$dose)
mae <- mean(absolute_errors)
print(paste("MAE =", sprintf(mae, fmt = "%#.4f")))
```
## 2. Logistic regression
### 2.a. Logistic Regression without caret ----
The glm() function is in the stats package and creates a
generalized linear model for regression or classification.
It can be configured to perform a logistic regression suitable for binary
classification problems.
```{r}
#### Load and split the dataset ----
library(readr)
diabetes <- read_csv("data/diabetes.csv")
View(diabetes)
# Define a 70:30 train:test data split of the dataset.
train_index <- createDataPartition(diabetes$Outcome,
                                   p = 0.7,
                                   list = FALSE)
diabetes_train <- diabetes[train_index, ]
diabetes_test <- diabetes[-train_index, ]

#### Train the model ----
diabetes_model_glm <- glm(Outcome ~ ., data = diabetes_train,
                          family = binomial(link = "logit"))

#### Display the model's details ----
print(diabetes_model_glm)

#### Make predictions ----
probabilities <- predict(diabetes_model_glm, diabetes_test[, 1:8],
                         type = "response")
print(probabilities)
predictions <- ifelse(probabilities > 0.5, "pos", "neg")
print(predictions)

#### Display the model's evaluation metrics ----
table(predictions, diabetes_test$Outcome)

# Read the following article on how to compute various evaluation metrics using
# the confusion matrix:
# https://en.wikipedia.org/wiki/Confusion_matrix
```
## 3. Linear Discriminant Analysis ----
### 3 Linear Discriminant Analysis without caret ----
The lda() function is in the MASS package and creates a linear model of a
multi-class classification problem.
```{r}
#### Load and split the dataset ----
ibrary(readr)
diabetes <- read_csv("data/diabetes.csv")
View(diabetes)
data("PimaIndiansDiabetes2")
# Define a 70:30 train:test data split of the dataset.
train_index <- createDataPartition(PimaIndiansDiabetes2$diabetes,
                                   p = 0.7,
                                   list = FALSE)
diabetes_train <- PimaIndiansDiabetes2[train_index, ]
diabetes_test <- PimaIndiansDiabetes2[-train_index, ]

#### Train the model ----
diabetes_lda <- lda(diabetes ~ ., data = diabetes_train)

#### Display the model's details ----
print(diabetes_lda)

#### Make predictions ----
predictions <- predict(diabetes_lda,
                       diabetes_test[, 1:8])$diabetes


table(predictions, diabetes_test$Outcome)

# Read the following article on how to compute various evaluation metrics using
# the confusion matrix:
# https://en.wikipedia.org/wiki/Confusion_matrix

```
## 4. Regularized Linear Regression ----
The glmnet() function is in the glmnet package and can be used for
 both classification and regression problems.
It can also be configured to perform three important types of regularization:
    1. lasso,
2. ridge and
3. elastic net
# by configuring the alpha parameter to 1, 0 or in [0,1] respectively.

### 4.a. Regularized Linear Regression Classification Problem without CARET
```{r}
library(readr)
diabetes <- read_csv("data/diabetes.csv")
View(diabetes)
x <- as.matrix(diabetes[, 1:8])
y <- as.matrix(diabetes[, 9])

#### Train the model ----
diabetes_model_glm <- glmnet(x, y, family = "binomial",
                             alpha = 0.5, lambda = 0.001)

#### Display the model's details ----
print(diabetes_model_glm)

#### Make predictions ----
predictions <- predict(diabetes_model_glm, x, type = "class")

#### Display the model's evaluation metrics ----
table(predictions, diabetes$Outcome)

### 4.b. Regularized Linear Regression Regression Problem without CARET ----
#### Load the dataset ----
library(readr)
diabetes <- read_csv("data/diabetes.csv")
View(diabetes)
x <- as.matrix(diabetes[, 1:8])
y <- as.matrix(diabetes[, 9])
diabetes$Outcome <- # nolint: object_name_linter.
  as.numeric(as.character(diabetes$Outcome))


#### Train the model ----
diabetes_model_glm <- glmnet(x, y, family = "gaussian",
                                   alpha = 0.5, lambda = 0.001)

#### Display the model's details ----
print(diabetes_model_glm)

#### Make predictions ----
predictions <- predict(diabetes_model_glm, x, type = "link")

#### Display the model's evaluation metrics ----
mse <- mean((y - predictions)^2)
print(mse)
##### RMSE ----
rmse <- sqrt(mean((y - predictions)^2))
print(paste("RMSE =", sprintf(rmse, fmt = "%#.4f")))

##### SSR ----
ssr <- sum((y - predictions)^2)
print(paste("SSR =", sprintf(ssr, fmt = "%#.4f")))

##### SST ----
sst <- sum((y - mean(y))^2)
print(paste("SST =", sprintf(sst, fmt = "%#.4f")))

##### R Squared ----
r_squared <- 1 - (ssr / sst)
print(paste("R Squared =", sprintf(r_squared, fmt = "%#.4f")))

##### MAE ----
absolute_errors <- abs(predictions - y)
mae <- mean(absolute_errors)
print(paste("MAE =", sprintf(mae, fmt = "%#.4f")))


```

# B. Non-Linear Algorithms ----
## 1.  Classification and Regression Trees ----
### 1.a. Decision tree for a classification problem without caret ----
```{r}
#### Load and split the dataset ----
library(readr)
breast_cancer <- read_csv("data/breast cancer.csv")
View(breast_cancer)
# Define a 70:30 train:test data split of the dataset.
train_index <- createDataPartition(breast_cancer$diagnosis,
                                   p = 0.7,
                                   list = FALSE)
breast_cancer_train <- breast_cancer[train_index, ]
breast_cancer_test <- breast_cancer[-train_index, ]

#### Train the model ----
breast_cancer_model_rpart <- rpart(diagnosis ~ ., data = breast_cancer_train)

#### Display the model's details ----
print(breast_cancer_model_rpart)

#### Make predictions ----
library(rpart)
predictions <- predict(breast_cancer_model_rpart,
                       breast_cancer_test[, 1:33],
                       type = "class")
class_predictions <- predict(breast_cancer_model_rpart, breast_cancer_test[, 1:33], 
                             type = 'class')

#### Display the model's evaluation metrics ----
table(predictions, breast_cancer_test$diagnosis)


```
### 1.c. Decision tree for a classification problem with caret ----
```{r}
#### Load and split the dataset ----
library(readr)
IRIS <- read_csv("data/IRIS.csv")
View(IRIS)

# Define a 70:30 train:test data split of the dataset.
train_index <- createDataPartition(IRIS$species,
                                   p = 0.7,
                                   list = FALSE)
IRIS_train <- IRIS[train_index, ]
IRIS_test <- IRIS[-train_index, ]

#### Train the model ----
set.seed(7)
# We apply the 5-fold cross validation resampling method
train_control <- trainControl(method = "cv", number = 5)
IRIS_caret_model_rpart <- train(species ~ ., data = IRIS,
                                    method = "rpart", metric = "Accuracy",
                                    trControl = train_control)

#### Display the model's details ----
print(IRIS_caret_model_rpart)
IRIS_model_rpart <- rpart(species ~ sepal_length + sepal_width + petal_length + petal_width, data = IRIS)

#### Make predictions ----
predictions <- predict(IRIS_model_rpart,
                       IRIS_test[, 1:5],
                       type = "class")

#### Display the model's evaluation metrics ----
table(predictions, IRIS_test$species)

```
### 1.d. Decision tree for a regression problem with CARET ----
```{r}
#### Load and split the dataset ----
library(readr)
Walmart <- read_csv("data/Walmart.csv")
View(Walmart)
# Define a 70:30 train:test data split of the dataset.
train_index <- createDataPartition(Walmart$Unemployment,
                                   p = 0.7,
                                   list = FALSE)
Walmart_train <- Walmart[train_index, ]
Walmart_test <- Walmart[-train_index, ]

#### Train the model ----
set.seed(7)
# 7 fold repeated cross validation with 3 repeats
train_control <- trainControl(method = "repeatedcv", number = 5, repeats = 3)

Walmart_caret_model_cart <- train(Unemployment ~ ., data = Walmart,
                                  method = "rpart", metric = "RMSE",
                                  trControl = train_control)
Walmart_model_rpart <- rpart(Unemployment ~ ., data = Walmart)


#### Display the model's details ----
print(Walmart_model_rpart)

#### Make predictions ----
predictions <- predict(Walmart_model_rpart, Walmart_test[, 1:7])

#### Display the model's evaluation metrics ----
##### RMSE ----
rmse <- sqrt(mean((Walmart_test$Unemployment - predictions)^2))
print(paste("RMSE =", sprintf(rmse, fmt = "%#.4f")))

##### SSR ----
# SSR is the sum of squared residuals (the sum of squared differences
# between observed and predicted values)
ssr <- sum((Walmart_test$Unemployment - predictions)^2)
print(paste("SSR =", sprintf(ssr, fmt = "%#.4f")))

##### SST ----
# SST is the total sum of squares (the sum of squared differences
# between observed values and their mean)
sst <- sum((Walmart_test$Unemployment - mean(Walmart_test$Unemployment))^2)
print(paste("SST =", sprintf(sst, fmt = "%#.4f")))

##### R Squared ----
# We then use SSR and SST to compute the value of R squared.
# The closer the R squared value is to 1, the better the model.
r_squared <- 1 - (ssr / sst)
print(paste("R Squared =", sprintf(r_squared, fmt = "%#.4f")))

##### MAE ----
# MAE is expressed in the same units as the target variable, making it easy to
# interpret. For example, if you are predicting the amount paid in rent,
# and the MAE is KES. 10,000, it means, on average, your model's predictions
# are off by about KES. 10,000.
absolute_errors <- abs(predictions - Walmart_test$Unemployment)
mae <- mean(absolute_errors)
print(paste("MAE =", sprintf(mae, fmt = "%#.4f")))
```
## 2.  Naïve Bayes ----
### 2.a. Naïve Bayes Classifier for a Classification Problem without CARET ----
We use the naiveBayes function inside the e1071 package
```{r}
#### Load and split the dataset ----
library(readr)
diabetes <- read_csv("data/diabetes.csv")
View(diabetes)

# Define a 70:30 train:test data split of the dataset.
train_index <- createDataPartition(diabetes$Outcome,
                                   p = 0.7,
                                   list = FALSE)
diabetes_train <- diabetes[train_index, ]
diabetes_test <- diabetes[-train_index, ]

#### Train the model ----
diabetes_model_nb <- naiveBayes(Outcome ~ .,
                                data = diabetes_train)

#### Display the model's details ----
print(diabetes_model_nb)

#### Make predictions ----
predictions <- predict(diabetes_model_nb,
                       diabetes_test[, 1:8])

#### Display the model's evaluation metrics ----

# Make sure both predictions and reference have the same factor levels
predictions <- factor(predictions, levels = levels(diabetes_test[, 1:9]$Outcome))

table(predictions, diabetes_test$Outcome)

```
### 2.b. Naïve Bayes Classifier for a Classification Problem with CARET ----
```{r}
data("PimaIndiansDiabetes2")
# Define a 70:30 train:test data split of the dataset.
train_index <- createDataPartition(PimaIndiansDiabetes2$diabetes,
                                   p = 0.7,
                                   list = FALSE)
PimaIndiansDiabetes2_train <- PimaIndiansDiabetes2[train_index, ]
PimaIndiansDiabetes2_test <- PimaIndiansDiabetes2[-train_index, ]

#### Train the model ----
# We apply the 5-fold cross validation resampling method
set.seed(7)
train_control <- trainControl(method = "cv", number = 5)
PimaIndiansDiabetes2_caret_model_nb <- train(diabetes ~ .,
                                 data = PimaIndiansDiabetes2_train,
                                 method = "nb", metric = "Accuracy",
                                 trControl = train_control)

#### Display the model's details ----
print(diabetes_caret_model_nb)

#### Make predictions ----
predictions <- predict(diabetes_caret_model_nb,
                       pima_indians_diabetes_test[, 1:8])



```
## 3.  k-Nearest Neighbours ----
## 3.  k-Nearest Neighbours ----
 The knn3() function is in the caret package and does not create a model.
Instead it makes predictions from the training dataset directly.
It can be used for classification or regression.
```{r}
library(readr)
diabetes <- read_csv("data/diabetes.csv")
View(diabetes)

# Define a 70:30 train:test data split of the dataset.
train_index <- createDataPartition(diabetes$Outcome,
                                   p = 0.7,
                                   list = FALSE)
diabetes_train <- diabetes[train_index, ]
diabetes_test <- diabetes[-train_index, ]

#### Train the model ----
diabetes_caret_model_knn <- knn3(Outcome ~ ., data = diabetes_train, k=3)

#### Display the model's details ----
print(diabetes_caret_model_knn)

#### Make predictions ----
predictions <- predict(diabetes_caret_model_knn,
                       diabetes_test[, 1:8],
                       type = "class")

#### Display the model's evaluation metrics ----
table(predictions, diabetes_test$Outcome)

# Or alternatively:
confusion_matrix <-
  caret::confusionMatrix(predictions,
                         diabetes_test$Outcome)
print(confusion_matrix)

fourfoldplot(as.table(confusion_matrix), color = c("grey", "lightblue"),
             main = "Confusion Matrix")

```
### 3.b. kNN for a regression problem without CARET's train function ----
```{r}
#### Load the dataset ----
library(readr)
diabetes <- read_csv("data/diabetes.csv")
View(diabetes)
diabetes$Outcome <- # nolint: object_name_linter.
  as.numeric(as.character(diabetes$Outcome))
x <- as.matrix(diabetes[, 1:8])
y <- as.matrix(diabetes[, 9])

#### Train the model ----
diabetes_model_knn <- knnreg(x, y, k = 3)

#### Display the model's details ----
print(diabetes_model_knn)

#### Make predictions ----
predictions <- predict(diabetes_model_knn, x)

#### Display the model's evaluation metrics ----
##### RMSE ----
rmse <- sqrt(mean((y - predictions)^2))
print(paste("RMSE =", sprintf(rmse, fmt = "%#.4f")))

##### SSR ----
ssr <- sum((y - predictions)^2)
print(paste("SSR =", sprintf(ssr, fmt = "%#.4f")))

##### SST ----
sst <- sum((y - mean(y))^2)
print(paste("SST =", sprintf(sst, fmt = "%#.4f")))

##### R Squared ----
r_squared <- 1 - (ssr / sst)
print(paste("R Squared =", sprintf(r_squared, fmt = "%#.4f")))

##### MAE ----
absolute_errors <- abs(predictions - y)
mae <- mean(absolute_errors)
print(paste("MAE =", sprintf(mae, fmt = "%#.4f")))

```
### 3.c. kNN for a classification problem with CARET's train function ----
```{r}
#### Load and split the dataset ----
library(readr)
diabetes <- read_csv("data/diabetes.csv")
View(diabetes)

# Define a 70:30 train:test data split of the dataset.
train_index <- createDataPartition(diabetes$Outcome,
                                   p = 0.7,
                                   list = FALSE)
diabetes_train <- diabetes[train_index, ]
diabetes_test <- diabetes[-train_index, ]

#### Train the model ----
# We apply the 10-fold cross validation resampling method
# We also apply the standardize data transform
set.seed(7)
train_control <- trainControl(method = "cv", number = 10)
diabetes_caret_model_knn <- train(Outcome ~ ., data = diabetes,
                                  method = "knn", metric = "Accuracy",
                                  preProcess = c("center", "scale"),
                                  trControl = train_control)
diabetes$Outcome <- factor(diabetes$Outcome, levels = c("0", "1"))
diabetes_caret_model_logreg <- train(Outcome ~ ., data = diabetes,
                                     method = "glm", family = "binomial",
                                     metric = "Accuracy", trControl = train_control)

#### Display the model's details ----
print(diabetes_caret_model_knn)

#### Make predictions ----
predictions <- predict(diabetes_caret_model_knn,
                       diabetes_test[, 1:8])

#### Display the model's evaluation metrics ----
confusion_matrix <-
  caret::confusionMatrix(predictions,
                         diabetes_test[, 1:9]$Outcome)
print(confusion_matrix)

fourfoldplot(as.table(confusion_matrix), color = c("grey", "lightblue"),
             main = "Confusion Matrix")

```
### 3.d. kNN for a regression problem with CARET's train function ----
```{r}
#### Load and split the dataset ----
data("BostonHousing2")

# Define an 80:20 train:test data split of the dataset.
train_index <- createDataPartition(BostonHousing2$medv,
                                   p = 0.8,
                                   list = FALSE)
BostonHousing2_train <- BostonHousing2[train_index, ]
BostonHousing2_test <- BostonHousing2[-train_index, ]

#### Train the model ----
# We apply the 5-fold cross validation resampling method
# We also apply the standardize data transform
set.seed(7)
train_control <- trainControl(method = "cv", number = 5)

diabetes$Outcome <- factor(diabetes$Outcome, levels = c("0", "1"))
diabetes_caret_model_logreg <- train(Outcome ~ ., data = diabetes,
                                     method = "glm", family = "binomial",
                                     metric = "Accuracy", trControl = train_control)
housing_caret_model_knn <- train(medv ~ ., data = BostonHousing2,
                                 method = "knn", metric = "RMSE",
                                 preProcess = c("center", "scale"),
                                 trControl = train_control)

#### Display the model's details ----
print(housing_caret_model_knn)

#### Make predictions ----
predictions <- predict(housing_caret_model_knn,
                       boston_housing_test[, 1:19])

#### Display the model's evaluation metrics ----
##### RMSE ----
rmse <- sqrt(mean((boston_housing_test$medv - predictions)^2))
print(paste("RMSE =", sprintf(rmse, fmt = "%#.4f")))

##### SSR ----
# SSR is the sum of squared residuals (the sum of squared differences
# between observed and predicted values)
ssr <- sum((boston_housing_test$medv - predictions)^2)
print(paste("SSR =", sprintf(ssr, fmt = "%#.4f")))

##### SST ----
# SST is the total sum of squares (the sum of squared differences
# between observed values and their mean)
sst <- sum((boston_housing_test$medv - mean(boston_housing_test$medv))^2)
print(paste("SST =", sprintf(sst, fmt = "%#.4f")))

##### R Squared ----
# We then use SSR and SST to compute the value of R squared.
# The closer the R squared value is to 1, the better the model.
r_squared <- 1 - (ssr / sst)
print(paste("R Squared =", sprintf(r_squared, fmt = "%#.4f")))

##### MAE ----
# MAE is expressed in the same units as the target variable, making it easy to
# interpret. For example, if you are predicting the amount paid in rent,
# and the MAE is KES. 10,000, it means, on average, your model's predictions
# are off by about KES. 10,000.
absolute_errors <- abs(predictions - boston_housing_test$medv)
mae <- mean(absolute_errors)
print(paste("MAE =", sprintf(mae, fmt = "%#.4f")))

```
## 4.  Support Vector Machine ----
### 4.a. SVM Classifier for a classification problem without CARET ----
```{r}
#### Load and split the dataset ----
library(readr)
diabetes <- read_csv("data/diabetes.csv")
View(diabetes)

# Define a 70:30 train:test data split of the dataset.
train_index <- createDataPartition(diabetes$Outcome,
                                   p = 0.7,
                                   list = FALSE)
diabetes_train <- diabetes[train_index, ]
diabetes_test <- diabetes[-train_index, ]

#### Train the model ----
diabetes_model_svm <- ksvm(Outcome ~ ., data = diabetes_train,
                           kernel = "rbfdot")

#### Display the model's details ----
print(diabetes_model_svm)

#### Make predictions ----
predictions <- predict(diabetes_model_svm, diabetes_test[, 1:8],
                       type = "response")

#### Display the model's evaluation metrics ----
table(predictions, diabetes_test$Outcome)

confusion_matrix <-
  caret::confusionMatrix(predictions,
                         diabetes_test[, 1:9]$Outcome)
print(confusion_matrix)

fourfoldplot(as.table(confusion_matrix), color = c("grey", "lightblue"),
             main = "Confusion Matrix")
```
### 4.b. SVM Classifier for a regression problem without CARET ----
```{r}
#### Load and split the dataset ----
data("BostonHousing2")

# Define an 80:20 train:test data split of the dataset.
train_index <- createDataPartition(BostonHousing2$medv,
                                   p = 0.8,
                                   list = FALSE)
BostonHousing2_train <- BostonHousing2[train_index, ]
BostonHousing2_test <- BostonHousing2[-train_index, ]

#### Train the model ----
housing_model_svm <- ksvm(medv ~ ., BostonHousing2_train, kernel = "rbfdot")

#### Display the model's details ----
print(housing_model_svm)

#### Make predictions ----
predictions <- predict(housing_model_svm, BostonHousing2_test)

#### Display the model's evaluation metrics ----
##### RMSE ----
rmse <- sqrt(mean((BostonHousing2_test$medv - predictions)^2))
print(paste("RMSE =", sprintf(rmse, fmt = "%#.4f")))

##### SSR ----
# SSR is the sum of squared residuals (the sum of squared differences
# between observed and predicted values)
ssr <- sum((BostonHousing2_test$medv - predictions)^2)
print(paste("SSR =", sprintf(ssr, fmt = "%#.4f")))

##### SST ----
# SST is the total sum of squares (the sum of squared differences
# between observed values and their mean)
sst <- sum((BostonHousing2_test$medv - mean(BostonHousing2_test$medv))^2)
print(paste("SST =", sprintf(sst, fmt = "%#.4f")))

##### R Squared ----
# We then use SSR and SST to compute the value of R squared.
# The closer the R squared value is to 1, the better the model.
r_squared <- 1 - (ssr / sst)
print(paste("R Squared =", sprintf(r_squared, fmt = "%#.4f")))

##### MAE ----
# MAE is expressed in the same units as the target variable, making it easy to
# interpret. For example, if you are predicting the amount paid in rent,
# and the MAE is KES. 10,000, it means, on average, your model's predictions
# are off by about KES. 10,000.
absolute_errors <- abs(predictions - BostonHousing2_test$medv)
mae <- mean(absolute_errors)
print(paste("MAE =", sprintf(mae, fmt = "%#.4f")))

```
### 4.c. SVM Classifier for a classification problem with CARET ----
# The SVM with Radial Basis kernel implementation can be used with caret for
# classification as follows:
```{r}
#### Load and split the dataset ----
library(readr)
diabetes <- read_csv("data/diabetes.csv")
View(diabetes)

# Define a 70:30 train:test data split of the dataset.
train_index <- createDataPartition(diabetes$Outcome,
                                   p = 0.7,
                                   list = FALSE)
diabetes_train <- diabetes[train_index, ]
diabetes_test <- diabetes[-train_index, ]

#### Train the model ----
set.seed(7)
train_control <- trainControl(method = "cv", number = 5)



diabetes_caret_model_svm_radial <- # nolint: object_length_linter.
  train(Outcome ~ ., data = diabetes_train, method = "svmRadial",
        metric = "Accuracy", trControl = train_control)

#### Display the model's details ----
print(diabetes_caret_model_svm_radial)

#### Make predictions ----
predictions <- predict(diabetes_caret_model_svm_radial,
                       diabetes_test[, 1:8])

#### Display the model's evaluation metrics ----
table(predictions, diabetes_test$Outcome)
confusion_matrix <-
  caret::confusionMatrix(predictions,
                         diabetes_test[, 1:9]$Outcome)
print(confusion_matrix)

fourfoldplot(as.table(confusion_matrix), color = c("grey", "lightblue"),
             main = "Confusion Matrix")


```
### 4.d. SVM classifier for a regression problem with CARET ----
```{r}
#### Load and split the dataset ----
data(BostonHousing2)

# Define an 80:20 train:test data split of the dataset.
train_index <- createDataPartition(BostonHousing2$medv,
                                   p = 0.8,
                                   list = FALSE)
BostonHousing2_train <- BostonHousing2[train_index, ]
BostonHousing2_test <- BostonHousing2[-train_index, ]

#### Train the model ----
set.seed(7)
train_control <- trainControl(method = "cv", number = 5)
housing_caret_model_svm_radial <-
  train(medv ~ ., data = BostonHousing2_train,
        method = "svmRadial", metric = "RMSE",
        trControl = train_control)

#### Display the model's details ----
print(housing_caret_model_svm_radial)

#### Make predictions ----
predictions <- predict(housing_caret_model_svm_radial,
                       BostonHousing2_test[, 1:19])

#### Display the model's evaluation metrics ----
##### RMSE ----
rmse <- sqrt(mean((BostonHousing2_test$medv - predictions)^2))
print(paste("RMSE =", sprintf(rmse, fmt = "%#.4f")))

##### SSR ----
# SSR is the sum of squared residuals (the sum of squared differences
# between observed and predicted values)
ssr <- sum((BostonHousing2_test$medv - predictions)^2)
print(paste("SSR =", sprintf(ssr, fmt = "%#.4f")))

##### SST ----
# SST is the total sum of squares (the sum of squared differences
# between observed values and their mean)
sst <- sum((BostonHousing2_test$medv - mean(BostonHousing2_test$medv))^2)
print(paste("SST =", sprintf(sst, fmt = "%#.4f")))

##### R Squared ----
# We then use SSR and SST to compute the value of R squared.
# The closer the R squared value is to 1, the better the model.
r_squared <- 1 - (ssr / sst)
print(paste("R Squared =", sprintf(r_squared, fmt = "%#.4f")))

##### MAE ----
# MAE is expressed in the same units as the target variable, making it easy to
# interpret. For example, if you are predicting the amount paid in rent,
# and the MAE is KES. 10,000, it means, on average, your model's predictions
# are off by about KES. 10,000.
absolute_errors <- abs(predictions - BostonHousing2_test$medv)
mae <- mean(absolute_errors)
print(paste("MAE =", sprintf(mae, fmt = "%#.4f")))
```




